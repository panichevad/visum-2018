{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "* Acknowledgment: This notebook is being used with the kind permission of Kelwin Fernandes and Ricardo Cruz\n",
    "\n",
    "We are going to use a linear regression for a small introduction.\n",
    "\n",
    "See the Linear Regression [documentation here.](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "A linear regression is a model of the type:\n",
    "\n",
    "$y = w\\cdot x + b$\n",
    "\n",
    "(where $w$ and $b$ are discovered automatically based on the data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invent our own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1000\n",
    "w0 = -5\n",
    "w1 = 1e-2\n",
    "w2 = -6e-6\n",
    "w3 = 4e-10\n",
    "\n",
    "x = np.linspace(100, 1000, 25)\n",
    "y = b + w0*x + w1*x*x + w2*x*x*x + w3*x*x*x*x\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "plt.xlabel('Area (m²)')\n",
    "plt.ylabel('Price (1000 €)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some random Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = b + w0*x + w1*x*x + w2*x*x*x + w3*x*x*x*x\n",
    "y += np.random.randn(len(x))*20\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "plt.xlabel('Area (m²)')\n",
    "plt.ylabel('Price (1000 €)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a linear regression:\n",
    "\n",
    "$Price = b + w_0Area$\n",
    "\n",
    "A linear regression will try to find the values of $b$ and $w_0$ which minimize the difference between the real $Price$ and the predicted Price.\n",
    "\n",
    "In mathematical terms, we want to $\\min_{b,w_0}\\|Price-\\hat{Price}\\|^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "m = LinearRegression()\n",
    "m.fit(x[:, np.newaxis], y)\n",
    "print('score:', m.score(x[:, np.newaxis], y))\n",
    "\n",
    "b = m.intercept_\n",
    "w0 = m.coef_[0]\n",
    "\n",
    "print(b, w0)\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, b+w0*x, '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is not expressive enough right?\n",
    "\n",
    "Let us try this model:\n",
    "\n",
    "$Price = b + w_0Area + w_1{Area}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "m = LinearRegression()\n",
    "_x = np.c_[x, x*x]\n",
    "m.fit(_x, y)\n",
    "print('score:', m.score(_x, y))\n",
    "\n",
    "b = m.intercept_\n",
    "w0 = m.coef_[0]\n",
    "w1 = m.coef_[1]\n",
    "print(b, w0, w1)\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, b+w0*x+w1*x*x, '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's better, but the coefficients are still very different than the original model...\n",
    "\n",
    "Let us try a model similar to the original model:\n",
    "\n",
    "$Price = b + w_0Area + w_1{Area}^2 + w_2{Area}^3 + w_3{Area}^4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 4\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "m = LinearRegression(normalize=True)\n",
    "_x = np.array([x**(i+1) for i in range(order)]).T\n",
    "m.fit(_x, y)\n",
    "print('score:', m.score(_x, y))\n",
    "\n",
    "b = m.intercept_\n",
    "ws = m.coef_\n",
    "print(b, ws)\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, b+np.sum([ws[i]*x**(i+1) for i in range(order)], 0), '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** change the previous code to use a polynomial of **order=40** !\n",
    "\n",
    "$Price = b + w_0Area + w_1{Area}^2 + w_2{Area}^3 + w_3{Area}^4 + \\dots + w_{37}{Area}^{38} + w_{38}{Area}^{39} + w_{39}{Area}^{40}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error in the training is smaller, but does this model explain better the numbers?\n",
    "\n",
    "Look at the coefficients... Will this model give good results in the real world?\n",
    "\n",
    "----\n",
    "\n",
    "The problem is that many times we do not know the structure of the problem. We do know that prices relative to area follow a polynomial of order 4.\n",
    "\n",
    "What we can do in these cases is to use a high order polynomial, and then punish coefficients that are **too big!**\n",
    "\n",
    "In mathematical terms, we want to $\\min_{b,\\vec{w}}\\|Price-\\hat{Price}\\|^2 + \\alpha\\sum_i\\|w_i\\|$.\n",
    "\n",
    "We will use the Lasso model for that. See here [the documentation.](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 40\n",
    "alpha = 100\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "m = Lasso(alpha=alpha, normalize=True, max_iter=100000)\n",
    "m.fit(np.array([x**(i+1) for i in range(order)]).T, y)\n",
    "\n",
    "b = m.intercept_\n",
    "ws = m.coef_\n",
    "print(b, ws)\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, b+np.sum([ws[i]*x**(i+1) for i in range(order)], 0), '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Active coefficients:\", (np.arange(len(ws)) + 1)[np.abs(ws) > 1e-10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** change the `alpha` hyperparameter and see how that changes the graphic. (e.g. 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is not as good as the 4-th order polynomial, but, if we do not know the structure of the data, it is better than using a very high polynomial.\n",
    "\n",
    "This technique is used a lot, including in neural networks, to make sure that simpler hypothesis are emphasized over very complex ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
